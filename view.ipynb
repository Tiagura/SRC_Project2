{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ipaddress\n",
    "import dns.resolver\n",
    "import dns.reversename\n",
    "import pygeoip\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "#data \n",
    "data_normal = pd.read_parquet('data0.parquet')\n",
    "data_attack = pd.read_parquet('test0.parquet')\n",
    "\n",
    "# Get the organization name for an IP address\n",
    "geo1=pygeoip.GeoIP('./GeoIP_DBs/GeoIP.dat')\n",
    "geo2=pygeoip.GeoIP('./GeoIP_DBs/GeoIPASNum.dat')\n",
    "\n",
    "def get_countryname(ip):\n",
    "    return geo1.country_name_by_addr(ip)\n",
    "\n",
    "# Verify if all ports are equal in data_normal vs data_attack\n",
    "if (data_normal['port'].unique() != data_attack['port'].unique()).all():\n",
    "    print(\"Different ports in data_normal vs data_attack\")\n",
    "\n",
    "# Verify if all protocols are equal in data_normal vs data_attack\n",
    "if (data_normal['proto'].unique() != data_attack['proto'].unique()).all():\n",
    "    print(\"Different protocols in data_normal vs data_attack\")\n",
    "\n",
    "# Put a label for each dst_ip with the country name\n",
    "data_normal['dst_country'] = data_normal['dst_ip'].apply(get_countryname)\n",
    "data_attack['dst_country'] = data_attack['dst_ip'].apply(get_countryname)\n",
    "\n",
    "def timestamp_to_hour(timestamp):\n",
    "    timestamp = timestamp/100\n",
    "    hours, remainder = divmod(timestamp, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSSIBLE BOTNET\n",
    "\n",
    "# ips com maior trafego TCP -> ips servirdores (normal e anomalo) -> diferenÃ§as de ips == botnet\n",
    "\n",
    "# get private ips with more TCP traffic from data_normal both src_ip and dst_ip\n",
    "server_ips_normal = data_normal.loc[((data_normal['src_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)) & (data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)))]\n",
    "# get private ip 192.168.100.* as dst_ip with alot of TCP traffic from data_normal and count the number of flows\n",
    "servers_normal = server_ips_normal.loc[(server_ips_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)) & (server_ips_normal['dst_ip'].apply(lambda x: x.startswith('192.168.100.')))].groupby(['dst_ip']).size().reset_index(name='counts')\n",
    "\n",
    "# get private ips with more TCP traffic from data_attack both src_ip and dst_ip\n",
    "server_ips_attack = data_attack.loc[((data_attack['src_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)) & (data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)))]\n",
    "# get private ip 192.168.100.* as dst_ip with alot of TCP traffic from data_attack and count the number of flows\n",
    "servers_attack = server_ips_attack.loc[(server_ips_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)) & (server_ips_attack['dst_ip'].apply(lambda x: x.startswith('192.168.100.')))].groupby(['dst_ip']).size().reset_index(name='counts')\n",
    "\n",
    "# get the difference between servers_normal and servers_attack\n",
    "servers_attack = pd.merge(servers_normal, servers_attack, on=['dst_ip'], how='outer')\n",
    "\n",
    "# get ips that have NaN in counts_normal \n",
    "botnet_suspects_flows = servers_attack[servers_attack['counts_x'].isnull()]\n",
    "\n",
    "# fill NaN with 0\n",
    "servers_attack = servers_attack.fillna(0)\n",
    "\n",
    "# new column with the % of difference between counts_normal and counts_attack\n",
    "servers_attack['diff'] = (servers_attack['counts_y'] - servers_attack['counts_x']) / servers_attack['counts_x']\n",
    " \n",
    "\n",
    "# check if there are communication between botnet_suspects ips and count the number of flows\n",
    "botnet_suspects_flows = data_attack.loc[(data_attack['src_ip'].isin(botnet_suspects_flows['dst_ip'])) & (data_attack['dst_ip'].isin(botnet_suspects_flows['dst_ip']))].groupby(['src_ip','dst_ip']).size().reset_index(name='counts')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Servers:\n",
    "- 192.168.100.224 -> got 200% more flows than normal on data_attack why? Read next markdown\n",
    "- 192.168.100.225 -> got 200% more flows than normal on data_attack why? Read next markdown\n",
    "- 192.168.100.231\n",
    "- 192.168.100.235\n",
    "- 192.168.100.238\n",
    "- 192.168.100.239\n",
    "- 192.168.100.240\n",
    "\n",
    "Botnet_suspect_ips:\n",
    "- 192.168.100.114\n",
    "- 192.168.100.30\n",
    "- 192.168.100.43\n",
    "- 192.168.100.89\n",
    "\n",
    "They all communicate with each other on data_attack but not on data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/3374406550.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  suspect_ips_flows['timestamp'] = suspect_ips_flows['timestamp'].apply(timestamp_to_hour)\n"
     ]
    }
   ],
   "source": [
    "# Attack to Servers\n",
    "\n",
    "# ON NORMAL DATA\n",
    "# get ips with flows to servers and count the number of flows\n",
    "ips_to_servers_normal = data_normal.loc[(data_normal['dst_ip'].isin(servers_normal['dst_ip']))].groupby(['src_ip']).size().reset_index(name='counts')\n",
    "\n",
    "# ON ATTACK DATA\n",
    "# get ips with flows to servers and count the number of flows\n",
    "ips_to_servers_attack = data_attack.loc[(data_attack['dst_ip'].isin(servers_normal['dst_ip']))].groupby(['src_ip']).size().reset_index(name='counts')\n",
    "\n",
    "# get the difference between ips_to_servers_normal and ips_to_servers_attack\n",
    "ips_to_servers_attack = pd.merge(ips_to_servers_normal, ips_to_servers_attack, on=['src_ip'], how='outer')\n",
    "# fill Nan with 0\n",
    "ips_to_servers_attack = ips_to_servers_attack.fillna(0)\n",
    "# calculate the rise of flows to servers in %\n",
    "ips_to_servers_attack['rise'] = (ips_to_servers_attack['counts_y'] - ips_to_servers_attack['counts_x']) / ips_to_servers_attack['counts_x'] * 100\n",
    "\n",
    "# keeo the ones with rise > 200% or rise is infinity and have more than 100 flows to servers in count_y\n",
    "ips_to_servers_attack = ips_to_servers_attack[((ips_to_servers_attack['rise'] > 200) | (ips_to_servers_attack['rise'] == float('inf'))) & (ips_to_servers_attack['counts_y'] > 2000)]\n",
    "\n",
    "# from the ips with 1000% rise get the flows to servers\n",
    "ips_to_servers_attack_flows = data_attack.loc[(data_attack['src_ip'].isin(ips_to_servers_attack['src_ip'])) & (data_attack['dst_ip'].isin(servers_normal['dst_ip']))].groupby(['src_ip','dst_ip']).size().reset_index(name='counts')\n",
    "# keep the ones with more than 1000 flows\n",
    "ips_to_servers_attack_flows = ips_to_servers_attack_flows[ips_to_servers_attack_flows['counts'] > 1000]\n",
    "\n",
    "# get unique ips from ips_to_servers_attack_flows\n",
    "suspect_ips = ips_to_servers_attack_flows['src_ip'].unique()\n",
    "# every suspect ip get all their traffic\n",
    "suspect_ips_flows = data_attack.loc[(data_attack['src_ip'].isin(suspect_ips))]\n",
    "# timestamp to hour\n",
    "suspect_ips_flows['timestamp'] = suspect_ips_flows['timestamp'].apply(timestamp_to_hour)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attack to Servers:\n",
    "- 192.168.100.176 \n",
    "- 192.168.100.188\n",
    "\n",
    "These IPs have 70k+ and 40k+ connections to servers respectively. A rise of 12736% and 3192% respectively compared to normal data.\n",
    "\n",
    "They have a lot of flows to 192.168.100.224 and 192.168.100.225 which are servers. \n",
    "\n",
    "All these flows are with UDP protocol and port 53. This is some DNS attack.\n",
    "\n",
    "They have udp packets with servers and then communicate with outside IPs with tcp on port 443.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUSPECTS DOWNLOADING FROM INTERNET\n",
    "\n",
    "# In normal data:\n",
    "# get flows from private ips to internet and count the number of flows\n",
    "download_normal = data_normal.loc[((data_normal['src_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)) & (data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False))].groupby(['src_ip','dst_ip']).size().reset_index(name='counts')\n",
    "download_normal = download_normal[download_normal['counts'] > 100]\n",
    "\n",
    "# In attack data:\n",
    "# get flows from private ips to internet and count the number of flows\n",
    "download_attack = data_attack.loc[((data_attack['src_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)) & (data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False))].groupby(['src_ip','dst_ip']).size().reset_index(name='counts')\n",
    "download_attack = download_attack[download_attack['counts'] > 100]\n",
    "\n",
    "# get the difference between download_normal and download_attack\n",
    "download_attack = pd.merge(download_normal, download_attack, on=['src_ip','dst_ip'], how='right')\n",
    "\n",
    "# calculate the rise of flows to servers in %\n",
    "download_attack['rise'] = (download_attack['counts_y'] - download_attack['counts_x']) / download_attack['counts_x'] * 100\n",
    "\n",
    "# keep the ones with rise > 200% or rise is infinity and have more than 100 flows to servers in count_y\n",
    "download_attack = download_attack[((download_attack['rise'] > 200) | (download_attack['rise'] == float('inf'))) & (download_attack['counts_y'] > 100)]\n",
    "\n",
    "# get down_bytes from pair src_ip and dst_ip in download_attack\n",
    "download_attack_flows = data_attack.loc[(data_attack['src_ip'].isin(download_attack['src_ip'])) & (data_attack['dst_ip'].isin(download_attack['dst_ip']))].groupby(['src_ip','dst_ip'])['down_bytes'].sum().reset_index(name='down_bytes')\n",
    "\n",
    "# merge download_attack_flows with download_attack\n",
    "download_attack = pd.merge(download_attack, download_attack_flows, on=['src_ip','dst_ip'], how='outer')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPs that generated 5x more private - private and private-public flows in data_attack than in data_normal:\n",
    "- 192.168.100.75\n",
    "- 192.168.100.118\n",
    "- 192.168.100.129\n",
    "- 192.168.100.133\n",
    "- 192.168.100.163\n",
    "\n",
    "They downloaded something from the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXFILTRATION?\n",
    "\n",
    "# ON DATA NORMAL\n",
    "# from the src_ip count the up_bytes to the internet\n",
    "up_bytes_normal = data_normal.loc[(data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['src_ip'])['up_bytes'].sum().reset_index(name='up_bytes')\n",
    "# from the src_ip count the number of flows to the internet\n",
    "flows_normal = data_normal.loc[(data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['src_ip']).size().reset_index(name='flows')\n",
    "# merge up_bytes and flows and get the average up_bytes per flow\n",
    "up_bytes_normal = pd.merge(up_bytes_normal, flows_normal, on=['src_ip'], how='inner')\n",
    "up_bytes_normal['up/flows'] = up_bytes_normal['up_bytes']/up_bytes_normal['flows']\n",
    "\n",
    "# ON DATA ATTACK\n",
    "# from the src_ip count the up_bytes to the internet\n",
    "up_bytes_attack = data_attack.loc[(data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['src_ip'])['up_bytes'].sum().reset_index(name='up_bytes')\n",
    "# from the src_ip count the number of flows to the internet\n",
    "flows_attack = data_attack.loc[(data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['src_ip']).size().reset_index(name='flows')\n",
    "# merge up_bytes and flows and get the average up_bytes per flow\n",
    "up_bytes_attack = pd.merge(up_bytes_attack, flows_attack, on=['src_ip'], how='inner')\n",
    "up_bytes_attack['up/flows'] = up_bytes_attack['up_bytes']/up_bytes_attack['flows']\n",
    "\n",
    "# merge up_bytes_normal and up_bytes_attack and get the difference between up/flows and get the ones that have a 99% increase\n",
    "up_bytes_diff = pd.merge(up_bytes_normal, up_bytes_attack, on=['src_ip'], how='inner')\n",
    "up_bytes_diff = up_bytes_diff[(up_bytes_diff['up/flows_y'] > (up_bytes_diff['up/flows_x']*1.99))]\n",
    "\n",
    "# get the dst_ip that have communication with the src_ip in up_bytes_diff and their org\n",
    "exfiltration = data_attack.loc[(data_attack['src_ip'].isin(up_bytes_diff['src_ip'])) & (data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)]\n",
    "exfiltration = exfiltration[['src_ip','dst_ip','timestamp']]\n",
    "exfiltration['org'] = exfiltration['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
    "exfiltration['timestamp'] = exfiltration['timestamp'].apply(timestamp_to_hour)\n",
    "exfiltration['dst_country'] = exfiltration['dst_ip'].apply(get_countryname)\n",
    "\n",
    "# from these src_ip get the up_bytes to the internet total from data_attack\n",
    "up_bytes_stat = data_attack.loc[(data_attack['src_ip'].isin(exfiltration['src_ip'])) & (data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['src_ip'])['up_bytes'].sum().reset_index(name='up_exfiltration')\n",
    "# from these src_ip get the up_bytes to the internet total from data_normal\n",
    "up_bytes_stat['up_normal'] = data_normal.loc[(data_normal['src_ip'].isin(exfiltration['src_ip'])) & (data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['src_ip'])['up_bytes'].sum().reset_index(name='up_normal')['up_normal']\n",
    "# new column with the difference between up_exfiltration and up_normal in percentage\n",
    "up_bytes_stat['diff'] = (up_bytes_stat['up_exfiltration'] - up_bytes_stat['up_normal'])/up_bytes_stat['up_normal']*100\n",
    "\n",
    "# keep the ones that transmitted more than 1GB\n",
    "up_bytes_stat = up_bytes_stat[up_bytes_stat['up_exfiltration'] > 1000000000]\n",
    "\n",
    "# get to know the dst_ip that have communication with the src_ip in up_bytes_stat and their org and dns\n",
    "exfiltration_stat = data_attack.loc[(data_attack['src_ip'].isin(up_bytes_stat['src_ip'])) & (data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)]\n",
    "\n",
    "# group by src_ip and dst_ip and count the number of flows\n",
    "exfiltration_stat = exfiltration_stat.groupby(['src_ip','dst_ip']).size().reset_index(name='counts')\n",
    "# from the data_attack get the up_bytes from the src_ip and dst_ip -> src_ip, dst_ip, up_bytes\n",
    "get_up_bytes = data_attack.loc[(data_attack['src_ip'].isin(exfiltration_stat['src_ip'])) & (data_attack['dst_ip'].isin(exfiltration_stat['dst_ip']))].groupby(['src_ip','dst_ip'])['up_bytes'].sum().reset_index(name='up_bytes')\n",
    "# merge get_up_bytes with exfiltration_stat\n",
    "exfiltration_stat = pd.merge(exfiltration_stat, get_up_bytes, on=['src_ip','dst_ip'], how='inner')\n",
    "# keep the ones that have more than 1GB\n",
    "exfiltration_stat = exfiltration_stat[exfiltration_stat['up_bytes'] > 1000000000]\n",
    "\n",
    "# get their org \n",
    "exfiltration_stat['org'] = exfiltration_stat['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
    "# get their country\n",
    "exfiltration_stat['dst_country'] = exfiltration_stat['dst_ip'].apply(get_countryname)   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These IPs comunicate with the internet got 2x up_bytes/flows in data_attack than in data_normal:\n",
    "- 192.168.100.194\n",
    "- 192.168.100.145\n",
    "- 192.168.100.112\n",
    "- 192.168.100.121\n",
    "- 192.168.100.29\n",
    "- 192.168.100.23\n",
    "\n",
    "Even tought they happen in 'normal' hours, it's still suspicious, looking futher into it:\n",
    "- 192.168.100.194 up_bytes rised by   356.2895661238 % in data_attack\n",
    "- 192.168.100.145 up_bytes rised by    21.282723578  % in data_attack\n",
    "- 192.168.100.112 up_bytes rised by 21857.1962912547 % in data_attack\n",
    "- 192.168.100.121 up_bytes rised by 16006.5805975834 % in data_attack\n",
    "- 192.168.100.29  up_bytes rised by  7171.1105271715 % in data_attack\n",
    "- 192.168.100.23  up_bytes rised by   359.4208205681 % in data_attack\n",
    "\n",
    "Only 3 updated more than 1Gb:\n",
    "- 192.168.100.112\n",
    "- 192.168.100.121\n",
    "- 192.168.100.29\n",
    "\n",
    "Very possible case of exfiltration\n",
    "The following IPs where receiving that data:\n",
    "- 142.250.184.184 - AS15169 GOOGLE\n",
    "- 13.107.42.39    - AS8068 MICROSOFT-CORP-MSN-AS-BLOCK\n",
    "- 13.107.42.27    - AS8068 MICROSOFT-CORP-MSN-AS-BLOCK\n",
    "\n",
    "The IPs all belong to the USA, helps explain why USA has more down_bytes in data_attack than in data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTRY STATISTICS\n",
    "\n",
    "# ON DATA NORMAL\n",
    "# get the number of flows has destination to each country\n",
    "normal_country = data_normal.loc[(data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['dst_country']).size().reset_index(name='flows_to_country')\n",
    "# get the number of up_bytes has destination to each country\n",
    "normal_country['up_bytes'] = data_normal.loc[(data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['dst_country'])['up_bytes'].sum().reset_index(name='up_bytes')['up_bytes']\n",
    "# get the number of down_bytes has destination to each country\n",
    "normal_country['down_bytes'] = data_normal.loc[(data_normal['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['dst_country'])['down_bytes'].sum().reset_index(name='down_bytes')['down_bytes']\n",
    "\n",
    "\n",
    "# make a mean between the number of flows to each country\n",
    "normal_country['% flows'] = normal_country['flows_to_country']/normal_country['flows_to_country'].sum()\n",
    "# make the mean between the up_bytes of one country and the flows to that country\n",
    "normal_country['up_bytes/flows'] = normal_country['up_bytes']/normal_country['flows_to_country']\n",
    "# make the mean between the down_bytes of one country and the flows to that country\n",
    "normal_country['down_bytes/flows'] = normal_country['down_bytes']/normal_country['flows_to_country']\n",
    "\n",
    "# ON DATA ATTACK\n",
    "# get the number of flows has destination to each country\n",
    "attack_country = data_attack.loc[(data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['dst_country']).size().reset_index(name='flows_to_country')\n",
    "# get the number of up_bytes has destination to each country\n",
    "attack_country['up_bytes'] = data_attack.loc[(data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['dst_country'])['up_bytes'].sum().reset_index(name='up_bytes')['up_bytes']\n",
    "# get the number of down_bytes has destination to each country\n",
    "attack_country['down_bytes'] = data_attack.loc[(data_attack['dst_ip'].apply(lambda x: ipaddress.ip_address(x).is_private)==False)].groupby(['dst_country'])['down_bytes'].sum().reset_index(name='down_bytes')['down_bytes']\n",
    "\n",
    "# make a mean between the number of flows to each country\n",
    "attack_country['% flows'] = attack_country['flows_to_country']/attack_country['flows_to_country'].sum()\n",
    "# make the mean between the up_bytes of one country and the flows to that country\n",
    "attack_country['up_bytes/flows'] = attack_country['up_bytes']/attack_country['flows_to_country']\n",
    "# make the mean between the down_bytes of one country and the flows to that country\n",
    "attack_country['down_bytes/flows'] = attack_country['down_bytes']/attack_country['flows_to_country']\n",
    "\n",
    "# merge normal_country and attack_country and get the difference between % flows, up_bytes/flows and down_bytes/flows and get the ones that have a 99% increase and more than 100 flows\n",
    "country_diff = pd.merge(normal_country, attack_country, on=['dst_country'], how='outer')\n",
    "# substitute Nan with 0\n",
    "country_diff = country_diff.fillna(0)\n",
    "country_diff = country_diff[((country_diff['% flows_y'] > (country_diff['% flows_x']*4.99)) | (country_diff['up_bytes/flows_y'] > (country_diff['up_bytes/flows_x']*1.99)) | (country_diff['down_bytes/flows_y'] > (country_diff['down_bytes/flows_x']*1.99))) & (country_diff['flows_to_country_y'] > 100)]\n",
    "\n",
    "# compare all x with all y make it a % rise\n",
    "country_diff['% flows rise'] = (country_diff['% flows_y'] - country_diff['% flows_x']) / country_diff['% flows_x'] * 100\n",
    "country_diff['up_bytes/flows rise'] = (country_diff['up_bytes/flows_y'] - country_diff['up_bytes/flows_x']) / country_diff['up_bytes/flows_x'] * 100\n",
    "country_diff['down_bytes/flows rise'] = (country_diff['down_bytes/flows_y'] - country_diff['down_bytes/flows_x']) / country_diff['down_bytes/flows_x'] * 100\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Countries:\n",
    "- China -> more than 5x more flows in data_attack than in data_normal, but up_bytes/flows and down_bytes/flows are similar between data_attack and data_normal\n",
    "- Russia -> it didn't happen in data_normal, but it happened in data_attack\n",
    "- USA -> alot more up_bytes and down_bytes in data_attack than in data_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7409/362552707.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_23['timestamp'] = ip_192_168_100_23['timestamp'].apply(timestamp_to_hour)\n",
      "/tmp/ipykernel_7409/362552707.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_23['org'] = ip_192_168_100_23['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
      "/tmp/ipykernel_7409/362552707.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_23['down_bytes'] = ip_192_168_100_23.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_23['up_bytes'] = ip_192_168_100_23.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_23['down_bytes_org'] = ip_192_168_100_23.groupby(['org'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_23['up_bytes_org'] = ip_192_168_100_23.groupby(['org'])['up_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_59['timestamp'] = ip_192_168_100_59['timestamp'].apply(timestamp_to_hour)\n",
      "/tmp/ipykernel_7409/362552707.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_59['org'] = ip_192_168_100_59['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
      "/tmp/ipykernel_7409/362552707.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_59['down_bytes'] = ip_192_168_100_59.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_59['up_bytes'] = ip_192_168_100_59.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_59['down_bytes_org'] = ip_192_168_100_59.groupby(['org'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_59['up_bytes_org'] = ip_192_168_100_59.groupby(['org'])['up_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_181['timestamp'] = ip_192_168_100_181['timestamp'].apply(timestamp_to_hour)\n",
      "/tmp/ipykernel_7409/362552707.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_181['org'] = ip_192_168_100_181['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
      "/tmp/ipykernel_7409/362552707.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_181['down_bytes'] = ip_192_168_100_181.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_181['up_bytes'] = ip_192_168_100_181.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_181['down_bytes_org'] = ip_192_168_100_181.groupby(['org'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:97: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_181['up_bytes_org'] = ip_192_168_100_181.groupby(['org'])['up_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_118['timestamp'] = ip_192_168_100_118['timestamp'].apply(timestamp_to_hour)\n",
      "/tmp/ipykernel_7409/362552707.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_118['org'] = ip_192_168_100_118['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
      "/tmp/ipykernel_7409/362552707.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_118['down_bytes'] = ip_192_168_100_118.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
      "/tmp/ipykernel_7409/362552707.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ip_192_168_100_118['up_bytes'] = ip_192_168_100_118.groupby(['src_ip'])['up_bytes'].transform('sum')\n"
     ]
    }
   ],
   "source": [
    "# TIME STATISTICS\n",
    "\n",
    "# ON DATA NORMAL\n",
    "# get src_ip and its min timestamp and max timestamp\n",
    "normal_time  = data_normal.groupby(['src_ip'])['timestamp'].agg(['min','max']).reset_index()\n",
    "\n",
    "# get the mean min timestamp and max timestamp between all src_ip\n",
    "mean_min_time = normal_time['min'].mean()\n",
    "mean_max_time = normal_time['max'].mean()\n",
    "\n",
    "# ON DATA ATTACK\n",
    "start_work = 7\n",
    "end_work = 22\n",
    "# get the src_ip and its min timestamp and max timestamp\n",
    "attack_time  = data_attack.groupby(['src_ip'])['timestamp'].agg(['min','max']).reset_index()\n",
    "attack_time['min'] = attack_time['min'].apply(timestamp_to_hour)\n",
    "attack_time['max'] = attack_time['max'].apply(timestamp_to_hour)\n",
    "\n",
    "# keep the ones that min_time is before 7:00:00 or max_time is after 22:00:00\n",
    "attack_time = attack_time[((attack_time['min'] < '07:00:00') | (attack_time['max'] > '22:00:00'))]\n",
    "\n",
    "# get number of total flows from data_attack\n",
    "attack_time['flows_anomaly'] = data_attack.groupby(['src_ip']).size().reset_index(name='flows')['flows']\n",
    "# get number of total flows from data_normal\n",
    "attack_time['flows_normal'] = data_normal.groupby(['src_ip']).size().reset_index(name='flows')['flows']\n",
    "# get the % of increase of flows\n",
    "attack_time['flows_rise'] = (attack_time['flows_anomaly'] - attack_time['flows_normal'])/attack_time['flows_normal']\n",
    "\n",
    "# get number of total up_bytes from data_attack\n",
    "attack_time['up_bytes_anomaly'] = data_attack.groupby(['src_ip'])['up_bytes'].sum().reset_index(name='up_bytes')['up_bytes']\n",
    "# get up_bytes per flow from data_attack\n",
    "attack_time['up_bytes/flows_anomaly'] = attack_time['up_bytes_anomaly']/attack_time['flows_anomaly']\n",
    "# get number of total up_bytes from data_normal\n",
    "attack_time['up_bytes_normal'] = data_normal.groupby(['src_ip'])['up_bytes'].sum().reset_index(name='up_bytes')['up_bytes']\n",
    "# get up_bytes per flow from data_normal\n",
    "attack_time['up_bytes/flows_normal'] = attack_time['up_bytes_normal']/attack_time['flows_normal']\n",
    "# get the % of increase of up_bytes/flows\n",
    "attack_time['up_bytes_rise'] = (attack_time['up_bytes/flows_anomaly'] - attack_time['up_bytes/flows_normal'])/attack_time['up_bytes/flows_normal']\n",
    "\n",
    "# get number of total down_bytes from data_attack\n",
    "attack_time['down_bytes_anomaly'] = data_attack.groupby(['src_ip'])['down_bytes'].sum().reset_index(name='down_bytes')['down_bytes']\n",
    "# get down_bytes per flow from data_attack\n",
    "attack_time['down_bytes/flows_anomaly'] = attack_time['down_bytes_anomaly']/attack_time['flows_anomaly']\n",
    "# get number of total down_bytes from data_normal\n",
    "attack_time['down_bytes_normal'] = data_normal.groupby(['src_ip'])['down_bytes'].sum().reset_index(name='down_bytes')['down_bytes']\n",
    "# get down_bytes per flow from data_normal\n",
    "attack_time['down_bytes/flows_normal'] = attack_time['down_bytes_normal']/attack_time['flows_normal']\n",
    "# get the % of increase of down_bytes/flows\n",
    "attack_time['down_bytes_rise'] = (attack_time['down_bytes/flows_anomaly'] - attack_time['down_bytes/flows_normal'])/attack_time['down_bytes/flows_normal']\n",
    "\n",
    "# keep the ones with flows_rise > 100% or up_bytes_rise > 50% or down_bytes_rise > 50%\n",
    "attack_time = attack_time[((attack_time['flows_rise'] > 1) | (attack_time['up_bytes_rise'] > 0.5) | (attack_time['down_bytes_rise'] > 0.5))]\n",
    "\n",
    "# get all flows from ip 192.168.100.23 in data_attack\n",
    "ip_192_168_100_23 = data_attack.loc[(data_attack['src_ip']=='192.168.100.23')]\n",
    "# timestamp to hour\n",
    "ip_192_168_100_23['timestamp'] = ip_192_168_100_23['timestamp'].apply(timestamp_to_hour)\n",
    "# get org from dst_ip\n",
    "ip_192_168_100_23['org'] = ip_192_168_100_23['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
    "# get the down_bytes total of the ip\n",
    "ip_192_168_100_23['down_bytes'] = ip_192_168_100_23.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes total of the ip\n",
    "ip_192_168_100_23['up_bytes'] = ip_192_168_100_23.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
    "# get the down_bytes of org\n",
    "ip_192_168_100_23['down_bytes_org'] = ip_192_168_100_23.groupby(['org'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes of org\n",
    "ip_192_168_100_23['up_bytes_org'] = ip_192_168_100_23.groupby(['org'])['up_bytes'].transform('sum')\n",
    "\n",
    "# get all flows from ip 192.168.100.59 in data_attack\n",
    "ip_192_168_100_59 = data_attack.loc[(data_attack['src_ip']=='192.168.100.59')]\n",
    "# timestamp to hour\n",
    "ip_192_168_100_59['timestamp'] = ip_192_168_100_59['timestamp'].apply(timestamp_to_hour)\n",
    "# get org from dst_ip\n",
    "ip_192_168_100_59['org'] = ip_192_168_100_59['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
    "# get the down_bytes total of the ip\n",
    "ip_192_168_100_59['down_bytes'] = ip_192_168_100_59.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes total of the ip\n",
    "ip_192_168_100_59['up_bytes'] = ip_192_168_100_59.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
    "# get the down_bytes of org\n",
    "ip_192_168_100_59['down_bytes_org'] = ip_192_168_100_59.groupby(['org'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes of org\n",
    "ip_192_168_100_59['up_bytes_org'] = ip_192_168_100_59.groupby(['org'])['up_bytes'].transform('sum')\n",
    "\n",
    "# get all flows from ip 192.168.100.181\n",
    "ip_192_168_100_181 = data_attack.loc[(data_attack['src_ip']=='192.168.100.181')]\n",
    "# timestamp to hour\n",
    "ip_192_168_100_181['timestamp'] = ip_192_168_100_181['timestamp'].apply(timestamp_to_hour)\n",
    "# get org from dst_ip\n",
    "ip_192_168_100_181['org'] = ip_192_168_100_181['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
    "# get the down_bytes total of the ip\n",
    "ip_192_168_100_181['down_bytes'] = ip_192_168_100_181.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes total of the ip\n",
    "ip_192_168_100_181['up_bytes'] = ip_192_168_100_181.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
    "# get the down_bytes of org\n",
    "ip_192_168_100_181['down_bytes_org'] = ip_192_168_100_181.groupby(['org'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes of org\n",
    "ip_192_168_100_181['up_bytes_org'] = ip_192_168_100_181.groupby(['org'])['up_bytes'].transform('sum')\n",
    "\n",
    "# get all flows from ip 192.168.100.118\n",
    "ip_192_168_100_118 = data_attack.loc[(data_attack['src_ip']=='192.168.100.118')]\n",
    "# timestamp to hour\n",
    "ip_192_168_100_118['timestamp'] = ip_192_168_100_118['timestamp'].apply(timestamp_to_hour)\n",
    "# get org from dst_ip\n",
    "ip_192_168_100_118['org'] = ip_192_168_100_118['dst_ip'].apply(lambda x: geo2.org_by_addr(x))\n",
    "# get the down_bytes total of the ip\n",
    "ip_192_168_100_118['down_bytes'] = ip_192_168_100_118.groupby(['src_ip'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes total of the ip\n",
    "ip_192_168_100_118['up_bytes'] = ip_192_168_100_118.groupby(['src_ip'])['up_bytes'].transform('sum')\n",
    "# get the down_bytes of org\n",
    "ip_192_168_100_118['down_bytes_org'] = ip_192_168_100_118.groupby(['org'])['down_bytes'].transform('sum')\n",
    "# get the up_bytes of org\n",
    "ip_192_168_100_118['up_bytes_org'] = ip_192_168_100_118.groupby(['org'])['up_bytes'].transform('sum')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ip 192.168.100.23 has flows starting at 6:11, suspicious, looking futher into it:\n",
    "- The flows occur with almost no time between them, bot like behaviour\n",
    "- These early flows are with the servers, and then it starts communicating with the internet\n",
    "\n",
    "Same behaviour for the rest of ips are somewhat alike (.23, .59, .181, .118)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
